{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/mahdialaei/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/mahdialaei/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/mahdialaei/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from gensim.models import Word2Vec\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import nltk\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import numpy as np\n",
    "import random\n",
    "import re\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\Movie Review Dataset.csv\")\n",
    "reviews = df['review'].astype(str).tolist()\n",
    "\n",
    "def preprocess_text(text):\n",
    "    \n",
    "    text = text.lower()\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    \n",
    "    words = word_tokenize(text)\n",
    "    \n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    words = [word for word in words if word not in stop_words]\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    words = [lemmatizer.lemmatize(word) for word in words]\n",
    "    \n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_reviews = [preprocess_text(review) for review in reviews]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Word2Vec(sentences=tokenized_reviews, vector_size=200, window=10, min_count=10, workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def review_to_vector(review, model):\n",
    "    vector = np.zeros(model.vector_size)\n",
    "    num_words = 0\n",
    "    for word in review:\n",
    "        if word in model.wv:\n",
    "            vector += model.wv[word]\n",
    "            num_words += 1\n",
    "    if num_words != 0:\n",
    "        vector /= num_words\n",
    "    return vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "review_vectors = [review_to_vector(review, model) for review in tokenized_reviews]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def k_means(vectors, k=2, max_iterations=100):\n",
    "    centroids = random.sample(vectors, k)\n",
    "    \n",
    "    for _ in range(max_iterations):\n",
    "        clusters = [[] for _ in range(k)]\n",
    "        \n",
    "        for vector in vectors:\n",
    "            distances = [np.linalg.norm(vector - centroid) for centroid in centroids]\n",
    "            closest_centroid = distances.index(min(distances))\n",
    "            clusters[closest_centroid].append(vector)\n",
    "        \n",
    "        new_centroids = []\n",
    "        for cluster in clusters:\n",
    "            new_centroids.append(np.mean(cluster, axis=0))\n",
    "        \n",
    "        if np.allclose(centroids, new_centroids):\n",
    "            break\n",
    "        \n",
    "        centroids = new_centroids\n",
    "\n",
    "    return centroids, clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "centroids, clusters = k_means(review_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_representative_words(centroid, model, topn=10):\n",
    "    closest_words = model.wv.similar_by_vector(centroid, topn=topn)\n",
    "    return [word for word, similarity in closest_words]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Centroid 1 representative words: ['movie', 'really', 'unimportant', 'goodit', 'endbut', 'good', 'frighten', 'relate', 'one', 'complain']\n",
      "Centroid 2 representative words: ['movie', 'really', 'complain', 'anyways', 'sucked', 'guess', 'honestly', 'actually', 'maybe', 'anyway']\n"
     ]
    }
   ],
   "source": [
    "for i, centroid in enumerate(centroids):\n",
    "    representative_words = find_representative_words(centroid, model)\n",
    "    print(f\"Centroid {i+1} representative words: {representative_words}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_cluster = clusters[0]\n",
    "second_cluster = clusters[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_cluster = [list(cluster) for cluster in first_cluster]\n",
    "second_cluster = [list(cluster) for cluster in second_cluster]\n",
    "review_vectors = [list(vectors) for vectors in review_vectors]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(first_cluster) > len(second_cluster):\n",
    "    idx_secondCl_reviews = [review_vectors.index(cluster) for cluster in second_cluster]\n",
    "else:\n",
    "    idx_firstCl_reviews = [review_vectors.index(cluster) for cluster in first_cluster]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(first_cluster) > len(second_cluster):\n",
    "    df[\"cluster_assignment\"] = 1\n",
    "else:\n",
    "    df[\"cluster_assignment\"] = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(first_cluster) > len(second_cluster):\n",
    "    df.loc[idx_secondCl_reviews, 'cluster_assignment'] = 2\n",
    "else:\n",
    "    df.loc[idx_firstCl_reviews, 'cluster_assignment'] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
